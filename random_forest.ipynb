{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We train a random forest classifier on the data set tumour_samples.csv employing cross-entropy as our information criterion for the splits in the decision trees. We use 5-fold cross-validation subsets to explore and optimise over suitable ranges the following hyper-parameters:\n",
    "\n",
    "1. number of decision trees\n",
    "\n",
    "2. depth of trees.\n",
    "\n",
    "We use accuracy as the measure of performance for this hyper-parameter optimisation.\n",
    "\n",
    "We compare the performance of your optimal random forest classifier on the data set tumour_samples.csv to the performance on the test data tumour_test.csv using different measures computed from the confusion matrix, in particular commenting on accuracy, recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we work with a dataset that doesn't have uniform weights, we create a function that computes the sample weights as follows\n",
    "\\begin{equation}\n",
    "\\frac{n}{c \\cdot b}\n",
    "\\end{equation}\n",
    "where $n$ is the number of samples, $c$ is the number of classes and $b$ counts how many times the label of the current point appears in $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_weights(y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    y: training responses\n",
    "    Output:\n",
    "    Returns the sample weights for the each entry in y.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels, counts = np.unique(y, return_counts=True)\n",
    "    class_weights = {}\n",
    "    sample_weights = []\n",
    "    \n",
    "    # Compute the weights for each class\n",
    "    for i in range(len(labels)):\n",
    "        class_weights[labels[i]] = 1 / counts[i] + len(y) / len(labels)\n",
    "    \n",
    "    # Assign a weight to each sample according to its label class\n",
    "    for i in range(len(y)):\n",
    "        sample_weights.append(class_weights[y[i]])\n",
    "\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function that computes the cross entropy given as \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^Q -p \\log(p)\n",
    "\\end{equation}\n",
    "where $p = \\mathbb P (y = c_i)$ is the weight of a sample divided by the total weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, sample_weights):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    y: vector of training labels, of shape (N,).\n",
    "    sample_weights: weights for each samples, of shape (N,).\n",
    "    Output:\n",
    "    The cross-entropy for y.\n",
    "    \"\"\"\n",
    "\n",
    "    # count different labels in yï¼Œand store in label_weights\n",
    "    # initialize with zero for each distinct label.\n",
    "    label_weights = {yi: 0 for yi in set(y)}  \n",
    "    for yi, wi in zip(y, sample_weights):\n",
    "        label_weights[yi] += wi\n",
    "\n",
    "    total_weight = sum(label_weights.values())\n",
    "    crossentropy = 0\n",
    "    \n",
    "    for label, weight in label_weights.items():\n",
    "        prob = weight / total_weight\n",
    "        crossentropy -= prob * np.log(prob)\n",
    "\n",
    "    return crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to split the data samples based on a feature (column) index and a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(X, y, sample_weights, column, value, categorical):\n",
    "  \"\"\"\n",
    "  Return the split of data whose column-th feature:\n",
    "    1. equals value, in case `column` is categorical, or\n",
    "    2. less than value, in case `column` is not categorical (i.e. numerical)\n",
    "\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      column: the column of the feature for splitting.\n",
    "      value: splitting threshold the samples \n",
    "      categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "  Returns:\n",
    "      tuple(np.array, np.array): tuple of subsets of X splitted based on column-th value.\n",
    "      tuple(np.array, np.array): tuple of subsets of y splitted based on column-th value.\n",
    "      tuple(np.array, np.array): tuple of subsets of sample weights based on column-th value.\n",
    "  \"\"\" \n",
    "\n",
    "  if categorical:\n",
    "    left_mask =(X[:, column] == value)\n",
    "  else:\n",
    "    left_mask = (X[:, column] < value)\n",
    "  \n",
    "  X_left, X_right = X[left_mask, :], X[~left_mask, :]\n",
    "  y_left, y_right = y[left_mask], y[~left_mask]\n",
    "  w_left, w_right  = sample_weights[left_mask], sample_weights[~left_mask]\n",
    "\n",
    "  return (X_left, X_right), (y_left, y_right), (w_left, w_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function that calculates the crossentropy based on the column we found with the split_value function that minimizes the crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_split_value(X, y, sample_weights, column, categorical):\n",
    "  \"\"\"\n",
    "  Calculate the cross-entropy based on `column` with the split that minimizes the cross-entropy.\n",
    "  Input:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      column: the column of the feature for calculating. 0 <= column < D\n",
    "      categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "  Output:\n",
    "      The resulted cross-entropy and the corresponding value used in splitting.\n",
    "  \"\"\"\n",
    "  \n",
    "  unique_vals = np.unique(X[:, column])\n",
    "\n",
    "  assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
    "\n",
    "  crossentropy_val, threshold = np.inf, None\n",
    "  \n",
    "  # split the values of i-th feature and calculate the cost \n",
    "  for value in unique_vals:\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, column, value, categorical)\n",
    "\n",
    "    # if one of the two sides is empty, skip this split.\n",
    "    if len(y_l) == 0 or len(y_r) == 0:\n",
    "      continue\n",
    "    \n",
    "    p_left = sum(w_l)/(sum(w_l) + sum(w_r))\n",
    "    p_right = 1 - p_left\n",
    "    new_cost = p_left * cross_entropy(y_l, w_l) + p_right * cross_entropy(y_r, w_r)\n",
    "    if new_cost < crossentropy_val:\n",
    "      crossentropy_val, threshold = new_cost, value\n",
    "    \n",
    "  return crossentropy_val, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_split_rf(n_features, X, y, sample_weights, columns_dict):\n",
    "  \"\"\"\n",
    "  Choose the best feature to split according to criterion.\n",
    "  Args:\n",
    "      n_features: number of sampled features.\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "  Returns:\n",
    "      The minimized cross-entropy, the best feature index and value used in splitting.\n",
    "  \"\"\"\n",
    "\n",
    "  columns = np.random.choice(list(columns_dict.keys()), n_features, replace=False)\n",
    "  columns_dict = {c: columns_dict[c] for c in columns}\n",
    "  min_cross_entropy_index, split_column, split_val = np.inf, 0, 0\n",
    "\n",
    "  for column, categorical in columns_dict.items():\n",
    "\n",
    "    # skip column if samples are not seperable by that column.\n",
    "    if len(np.unique(X[:, column])) < 2:\n",
    "      continue\n",
    "\n",
    "    # search for the best splitting value for the given column.\n",
    "    cross_entropy_index, val = cross_entropy_split_value(X, y, sample_weights, column, categorical)    \n",
    "    \n",
    "    if cross_entropy_index < min_cross_entropy_index:\n",
    "        min_cross_entropy_index, split_column, split_val = cross_entropy_index, column, val\n",
    "\n",
    "  return min_cross_entropy_index, split_column, split_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a function that returns the label which appears the most in our label variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(y, sample_weights):\n",
    "  \"\"\"\n",
    "  Input:\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Output:\n",
    "      Return the label which appears the most in y.\n",
    "  \"\"\"\n",
    "\n",
    "  majority_label = {yi: 0 for yi in set(y)}\n",
    "\n",
    "  for y_i, w_i in zip(y, sample_weights):\n",
    "    majority_label[y_i] += w_i\n",
    "\n",
    "  return max(majority_label, key=majority_label.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_rf(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf=2):\n",
    "  \"\"\"Build the decision tree according to the data.\n",
    "  Args:\n",
    "      X: (np.array) training features, of shape (N, D).\n",
    "      y: (np.array) vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "      feature_names (list): record the name of features in X in the original dataset.\n",
    "      depth (int): current depth for this node.\n",
    "  Returns:\n",
    "      (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "        1. 'feature_name': The column name of the split.\n",
    "        2. 'feature_index': The column index of the split.\n",
    "        3. 'value': The value used for the split.\n",
    "        4. 'categorical': indicator for categorical/numerical variables.\n",
    "        5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
    "        6. 'left': The left sub-tree with the same structure.\n",
    "        7. 'right' The right sub-tree with the same structure.\n",
    "      Example:\n",
    "          mytree = {\n",
    "              'feature_name': 'petal length (cm)',\n",
    "              'feature_index': 2,\n",
    "              'value': 3.0,\n",
    "              'categorical': False,\n",
    "              'majority_label': None,\n",
    "              'left': {\n",
    "                  'feature_name': str,\n",
    "                  'feature_index': int,\n",
    "                  'value': float,\n",
    "                  'categorical': bool,\n",
    "                  'majority_label': None,\n",
    "                  'left': {..etc.},\n",
    "                  'right': {..etc.}\n",
    "              }\n",
    "              'right': {\n",
    "                  'feature_name': str,\n",
    "                  'feature_index': int,\n",
    "                  'value': float,\n",
    "                  'categorical': bool,\n",
    "                  'majority_label': None,\n",
    "                  'left': {..etc.},\n",
    "                  'right': {..etc.}\n",
    "              }\n",
    "          }\n",
    "  \"\"\"\n",
    "  # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
    "  if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf: \n",
    "      return {'majority_label': majority_vote(y, sample_weights)}\n",
    "  \n",
    "  else:\n",
    "    CE, split_index, split_val = cross_entropy_split_rf(n_features,\n",
    "                                                        X,\n",
    "                                                        y,\n",
    "                                                        sample_weights,\n",
    "                                                        columns_dict)\n",
    "    \n",
    "    # If CE is infinity, it means that samples are not seperable by the sampled features.\n",
    "    if CE == np.inf:\n",
    "      return {'majority_label': majority_vote(y, sample_weights)}\n",
    "    \n",
    "    categorical = columns_dict[split_index]\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X,\n",
    "                                                       y,\n",
    "                                                       sample_weights,\n",
    "                                                       split_index,\n",
    "                                                       split_val,\n",
    "                                                       categorical)\n",
    "    return {'feature_name': feature_names[split_index],\n",
    "            'feature_index': split_index,\n",
    "            'value': split_val,\n",
    "            'categorical': categorical,\n",
    "            'majority_label': None,\n",
    "            'left': build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
    "            'right': build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(B, n_features, X, y, columns_dict, max_depth, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Build the decision tree according to the training data.\n",
    "  Args:\n",
    "      B: number of decision trees.\n",
    "      X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n",
    "      y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "      sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "  else:\n",
    "      sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "  feature_names = X.columns.tolist()\n",
    "  X = X.to_numpy()\n",
    "  y = y.to_numpy()\n",
    "  N = X.shape[0]\n",
    "  training_indices = np.arange(N)\n",
    "  trees = []\n",
    "\n",
    "  for _ in range(B):\n",
    "    sample = np.random.choice(training_indices, N, replace=True)\n",
    "    X_sample = X[sample, :]\n",
    "    y_sample = y[sample]\n",
    "    w_sample = sample_weights[sample]\n",
    "    tree = build_tree_rf(n_features,\n",
    "                         X_sample,\n",
    "                         y_sample,\n",
    "                         w_sample,\n",
    "                         columns_dict,\n",
    "                         feature_names,\n",
    "                         depth=1,\n",
    "                         max_depth=max_depth)\n",
    "    trees.append(tree)\n",
    "\n",
    "  return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to use this fitted decision tree to make predictions for our test set `X_test`. To do so, we first define a function `classify` that takes each single data point `x` as an argument. We will write a wrapper function `predict` that calls this `classify` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x):\n",
    "  \"\"\"\n",
    "  Classify a single sample with the fitted decision tree.\n",
    "  Args:\n",
    "      x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
    "  Returns:\n",
    "      (int): predicted testing sample label.\n",
    "  \"\"\"\n",
    "  if tree['majority_label'] is not None: \n",
    "    return tree['majority_label']\n",
    "\n",
    "  elif tree['categorical']:\n",
    "    if x[tree['feature_index']] == tree['value']:\n",
    "      return classify(tree['left'], x)\n",
    "    else:\n",
    "      return classify(tree['right'], x)\n",
    "\n",
    "  else:\n",
    "    if x[tree['feature_index']] < tree['value']:\n",
    "      return classify(tree['left'], x)\n",
    "    else:\n",
    "      return classify(tree['right'], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the prediction function which aggregates the decision from all decision trees and returns the class with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(rf, X):\n",
    "  \"\"\"\n",
    "  Predict classification results for X.\n",
    "  Args:\n",
    "      rf: A trained random forest through train_rf function.\n",
    "      X: (pd.Dataframe) testing sample features, of shape (N, D).\n",
    "  Returns:\n",
    "      (np.array): predicted testing sample labels, of shape (N,).\n",
    "  \"\"\"\n",
    "\n",
    "  def aggregate(decisions):\n",
    "    count = defaultdict(int)\n",
    "    for decision in decisions:\n",
    "      count[decision] += 1\n",
    "    return max(count, key=count.get)\n",
    "\n",
    "  if len(X.shape) == 1:\n",
    "      return aggregate([classify(tree, X) for tree in rf])\n",
    "  else:\n",
    "      return np.array([aggregate([classify(tree, x) for tree in rf]) for x in X.to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function that computes the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_score(rf, X_test, y_test):\n",
    "  \"\"\"\n",
    "  Input:\n",
    "  rf: trained random forest\n",
    "  X_test: testing features\n",
    "  y_test: testing labels\n",
    "  Output:\n",
    "  Returns the accuracy. \n",
    "  \"\"\"\n",
    "\n",
    "  y_pred = predict_rf(rf, X_test)\n",
    "  return np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write a function that performs a 5-fold cross-validation tuning two parameters: B - the number of trees and max_depth - the maximum allowed depth of a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score_fr(data, folds, B, max_depth):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    data: the whole data\n",
    "    folds: the indices of the 5 folds\n",
    "    B: number of decision trees\n",
    "    max_depth: the maximum depth for the tree\n",
    "    Output:\n",
    "    Mean score from validation sets over the 5 folds.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(len(folds)):\n",
    "        val_indexes = folds[i]\n",
    "        train_indexes = list(set(range(data.shape[0])) - set(val_indexes))\n",
    "    \n",
    "        X_train_i = data.loc[train_indexes, data.columns[: -1]]\n",
    "        y_train_i = data.loc[train_indexes, data.columns[-1]]\n",
    "\n",
    "        columns_dict = {index: False for index in range(X_train_i.shape[1])}\n",
    "        # Take a third of the features as a general rule\n",
    "        n_features = X_train_i.shape[1] // 3 \n",
    "        sample_weights = calculate_sample_weights(y_train_i.to_numpy())\n",
    "\n",
    "        X_val_i = data.loc[val_indexes, data.columns[: -1]]\n",
    "        y_val_i = data.loc[val_indexes, data.columns[-1]]\n",
    "        rf = train_rf(B,\n",
    "                      n_features,\n",
    "                      X_train_i,\n",
    "                      y_train_i,\n",
    "                      columns_dict,\n",
    "                      max_depth,\n",
    "                      sample_weights)\n",
    "\n",
    "        score_i = rf_score(rf, X_val_i, y_val_i)\n",
    "        scores.append(score_i)\n",
    "\n",
    "    # Return the average score\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function that finds the optimal values for B and max_depth. We will store the score values for each B and max_depth in a matrix with shape (len(B_range), len(max_depth_range))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_parameters(data, folds, B_range, max_depth_range):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    data: the whole data\n",
    "    folds: the indices of the 5 folds\n",
    "    B_range: the range for the number of decision trees\n",
    "    max_depth_range: the range for the maximum depth for the tree\n",
    "    Output:\n",
    "    Returns the optimal values for B and max_depth and the optimal score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a matrix that will contain all scores\n",
    "    scores = np.zeros((len(B_range), len(max_depth_range)))\n",
    "\n",
    "    # Loop over B_range and max_depth_range\n",
    "    for i, B in enumerate(B_range):\n",
    "        for j, max_depth in enumerate(max_depth_range):\n",
    "            # Store the scores\n",
    "            scores[i][j] = cross_validation_score_fr(data, folds, B, max_depth)\n",
    "\n",
    "    # Find the optimal score\n",
    "    best_parameters_index = np.argmax(scores)\n",
    "\n",
    "    # Find the indices of the optimal B and optimal max_depth\n",
    "    B_optimal_index = best_parameters_index//len(max_depth_range)\n",
    "    max_depth_optimal_index = best_parameters_index%len(max_depth_range)\n",
    "\n",
    "    # Return the optimal values for B and max_depth and the optimal score\n",
    "    return B_range[B_optimal_index], max_depth_range[max_depth_optimal_index], scores[B_optimal_index][max_depth_optimal_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(N, num_folds):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    N: length of dataset\n",
    "    num_folds: number of folds xD\n",
    "    Output:\n",
    "    Splits the data into folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    fold_size = N // num_folds\n",
    "    index_perm = np.random.permutation(np.arange(N))\n",
    "    folds = []\n",
    "    for k in range(num_folds):\n",
    "        folds.append(index_perm[k*fold_size:(k+1)*fold_size])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import whole samples data\n",
    "tumour_samples = pd.read_csv(\"tumour_samples.csv\")\n",
    "tumour_samples = tumour_samples.drop(columns=['Unnamed: 0'])\n",
    "diag_map = {'M': 1.0, 'B': 0.0}\n",
    "tumour_samples['DIAGNOSIS'] = tumour_samples['DIAGNOSIS'].map(diag_map)\n",
    "\n",
    "# Create folds\n",
    "folds = cross_validation_split(tumour_samples.shape[0], 5)\n",
    "\n",
    "# Compute the optimal values for B and max_depth and the optimal score\n",
    "optimal_B, optimal_max_depth, optimal_score = choose_best_parameters(tumour_samples, \n",
    "                                                                     folds,\n",
    "                                                                     B_range=np.arange(3,6),\n",
    "                                                                     max_depth_range=np.arange(3, 6))\n",
    "print(\"Optimal number of trees:\", optimal_B)\n",
    "print(\"Optimal max_depth of a tree:\", optimal_max_depth)\n",
    "print(\"Score of the optimal number of trees and max_depth: \", optimal_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fix the hyper-parameters to be the optimal values found above and calculate the accuracy on the entire training set as well as the accuracy on the unseed testing dataset. We need to train a random forest in a similar way to our cross_validation_score_fr function but with the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "X_samples_rf = tumour_samples.loc[:, tumour_samples.columns[:-1]]\n",
    "y_samples_rf = tumour_samples.loc[:, tumour_samples.columns[-1]]\n",
    "\n",
    "# Define the colums dictionary\n",
    "columns_dict = {index: False for index in range(X_samples_rf.shape[1])}\n",
    "\n",
    "# Take the number of features to be 1/3 of the total features\n",
    "n_features = X_samples_rf.shape[1] // 3\n",
    "\n",
    "# Calculate the sample weights for the training lables\n",
    "sample_weights = calculate_sample_weights(y_samples_rf.to_numpy())\n",
    "\n",
    "# Build and train the random forest with the optimal parameters\n",
    "rf = train_rf(optimal_B,\n",
    "              n_features,\n",
    "              X_samples_rf,\n",
    "              y_samples_rf,\n",
    "              columns_dict,\n",
    "              optimal_max_depth,\n",
    "              sample_weights)\n",
    "# Compute the accuracy on the training set\n",
    "rf_samples_score = rf_score(rf, X_samples_rf, y_samples_rf)\n",
    "\n",
    "print(\"The accuracy on the training set is:\", rf_samples_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing data\n",
    "tumour_test = pd.read_csv(\"tumour_test.csv\")\n",
    "tumour_test = tumour_test.drop(columns=['Unnamed: 0'])\n",
    "tumour_test['DIAGNOSIS'] = tumour_test['DIAGNOSIS'].map(diag_map)\n",
    "X_test_rf = tumour_test.loc[:, tumour_test.columns[:-1]]\n",
    "y_test_rf = tumour_test.loc[:, tumour_test.columns[-1]]\n",
    "\n",
    "# Compute the accuracy on the testing set\n",
    "rf_test_score = rf_score(rf, X_test_rf, y_test_rf)\n",
    "\n",
    "print(\"The accuracy on the testing set is:\", rf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy on the testing set is slightly lower than the score on the training set, it is still very high which suggests that the model did not overfit very much and generalized well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create several functions to produce the following measures: The Confusion matrix, the True positive rate (recall), the true negative rate, the precision and the F1-score, given by\n",
    "\\begin{align*}\n",
    "&TPR = \\frac{TP}{TP+FN} = recall\\\\\n",
    "&TNR = \\frac{TN}{TN + FP}\\\\\n",
    "&precision = \\frac{TP}{TP+FP}\\\\\n",
    "&Fscore = 2 \\frac{precision \\cdot recall}{precision + recall}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    actual: the true labels\n",
    "    predicted: the predicted labels\n",
    "    Output:\n",
    "    The confusion matrix for this data.\n",
    "    \"\"\"\n",
    "    # extract the different classes\n",
    "    classes = np.unique(actual)\n",
    "\n",
    "    # initialize the confusion matrix\n",
    "    confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "    # loop across the different combinations of actual / predicted classes\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "\n",
    "           # count the number of instances in each combination of actual / predicted classes\n",
    "           confusion_matrix[i, j] = np.sum((actual == classes[i]) & (predicted == classes[j]))\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(actual, predicted):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    actual: the true labels\n",
    "    predicted: the predicted labels\n",
    "    Output:\n",
    "    The recall measure.\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matr = confusion_matrix(actual, predicted)\n",
    "    TP = confusion_matr[1,1]\n",
    "    FN = confusion_matr[1,0]\n",
    "    return TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_negative_rate(actual, predicted):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    actual: the true labels\n",
    "    predicted: the predicted labels\n",
    "    Output:\n",
    "    The true negative rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    confusion_matr = confusion_matrix(actual, predicted)\n",
    "    TN = confusion_matr[0,0]\n",
    "    FP = confusion_matr[1,0]\n",
    "    return TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actual, predicted):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    actual: the true labels\n",
    "    predicted: the predicted labels\n",
    "    Output:\n",
    "    The precision.\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matr = confusion_matrix(actual, predicted)\n",
    "    TP = confusion_matr[1,1]\n",
    "    FP = confusion_matr[0,1]\n",
    "    return TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(actual, predicted):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    actual: the true labels\n",
    "    predicted: the predicted labels\n",
    "    Output:\n",
    "    The F score.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = precision(actual, predicted)\n",
    "    r = recall(actual, predicted)\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions, we use our predict_rf function to create a prediction on both the sample and test datasets. Then we plot the confusion matrix for both and the other measures as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on sample data\n",
    "y_samples_rf_prediction = predict_rf(rf, X_samples_rf)\n",
    "# Prediction on test data\n",
    "y_test_rf_prediction = predict_rf(rf, X_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the two confusion matrices\n",
    "confusion_matrix_samples = confusion_matrix(y_samples_rf, y_samples_rf_prediction)\n",
    "confusion_matrix_test = confusion_matrix(y_test_rf, y_test_rf_prediction)\n",
    "\n",
    "# Plot confusion matrix for the sample data\n",
    "plt.matshow(confusion_matrix_samples, cmap=plt.cm.Blues, alpha = 0.5)\n",
    "for i in range(confusion_matrix_samples.shape[0]):\n",
    "    for j in range(confusion_matrix_samples.shape[1]):\n",
    "        plt.text(x=j, y=i, s=int(confusion_matrix_samples[i, j]))\n",
    " \n",
    "plt.xlabel(\"Predicted\", size=15)\n",
    "plt.ylabel(\"Actual\", size=15)\n",
    "plt.title(\"Confusion matrix for sample data\", size=15)\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for the test data\n",
    "plt.matshow(confusion_matrix_test, cmap=plt.cm.Greens, alpha = 0.7)\n",
    "for i in range(confusion_matrix_test.shape[0]):\n",
    "    for j in range(confusion_matrix_test.shape[1]):\n",
    "        plt.text(x=j, y=i, s=int(confusion_matrix_test[i, j]))\n",
    " \n",
    "plt.xlabel(\"Predicted\", size=15)\n",
    "plt.ylabel(\"Actual\", size=15)\n",
    "plt.title(\"Confusion matrix for test data\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the other four measures for the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the samples data\n",
    "recall_samples = recall(y_samples_rf, y_samples_rf_prediction)\n",
    "TNR_samples = true_negative_rate(y_samples_rf, y_samples_rf_prediction)\n",
    "precision_samples = precision(y_samples_rf, y_samples_rf_prediction)\n",
    "f_score_samples = f_score(y_samples_rf, y_samples_rf_prediction)\n",
    "\n",
    "# For the test data\n",
    "recall_test = recall(y_test_rf, y_test_rf_prediction)\n",
    "TNR_test = true_negative_rate(y_test_rf, y_test_rf_prediction)\n",
    "precision_test = precision(y_test_rf, y_test_rf_prediction)\n",
    "f_score_test = f_score(y_test_rf, y_test_rf_prediction)\n",
    "\n",
    "# Create a dictionary with all the measures\n",
    "data = {\"Dataset\": [\"Samples\", \"Test\"],\n",
    "        \"Recall\": [recall_samples, recall_test],\n",
    "        \"TNR\": [TNR_samples, TNR_test],\n",
    "        \"Precision\": [precision_samples, precision_test],\n",
    "        \"F score\": [f_score_samples, f_score_test]}\n",
    "\n",
    "# Create table\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the true negative rate is higher for the samples dataset, the recall, precision and F-score measures are higher on the unseen testing dataset which indicates that our model didn't overfit and generalized well (note that while it is odd that the scores are hihger on the test dataset, this may be simply )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58dbfdb34cf82127b32c5737e6183911655ff227e5c11e8f5e4b25048ae98ef2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
